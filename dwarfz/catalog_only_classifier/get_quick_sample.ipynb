{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# What do I want?\n",
    "\n",
    "Previously in `HSC_COSMOS_filtering.ipynb` I tested out some basic classifiers to get a smaller sample set, while still keeping completeness high.  I tested two basic classifiers: a RandomForest classifier and a Logistic Regression classifier.\n",
    "\n",
    "For my training data, I started by getting objects and labels from COSMOS. For input features, I then matched those COSMOS galaxies to their nearest HSC counterpart. I then used HSC i-band magnitude, along with HSC g-r, r-i, i-z, z-y colors.\n",
    "\n",
    "Choosing some arbitrary thresholds, I got similar results for the Random Forest and the Logistic Regression classifiers. In this notebook I'll look at the full ROC curves for both classifiers, in hopes of better understanding my results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# give access to importing dwarfz\n",
    "import os, sys\n",
    "dwarfz_package_dir = os.getcwd().split(\"dwarfz\")[0]\n",
    "if dwarfz_package_dir not in sys.path:\n",
    "    sys.path.insert(0, dwarfz_package_dir)\n",
    "\n",
    "import dwarfz\n",
    "    \n",
    "# back to regular import statements\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "COSMOS_filename = os.path.join(dwarfz.data_dir_default, \"COSMOS_reference.sqlite\")\n",
    "COSMOS = dwarfz.datasets.COSMOS(COSMOS_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "HSC_filename = os.path.join(dwarfz.data_dir_default, \"HSC_COSMOS_median_forced.sqlite3\")\n",
    "HSC = dwarfz.datasets.HSC(HSC_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "COSMOS.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "HSC.df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "matches_filename = os.path.join(dwarfz.data_dir_default, \"matches.sqlite3\")\n",
    "matches_df = dwarfz.matching.Matches.load_from_filename(matches_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "combined = matches_df[matches_df.match].copy()\n",
    "combined[\"ra\"]       = COSMOS.df.loc[combined.index].ra\n",
    "combined[\"dec\"]      = COSMOS.df.loc[combined.index].dec\n",
    "combined[\"photo_z\"]  = COSMOS.df.loc[combined.index].photo_z\n",
    "combined[\"log_mass\"] = COSMOS.df.loc[combined.index].mass_med\n",
    "\n",
    "photometry_cols = [\n",
    "    \"gcmodel_flux\",\"gcmodel_flux_err\",\"gcmodel_flux_flags\",\n",
    "    \"rcmodel_flux\",\"rcmodel_flux_err\",\"rcmodel_flux_flags\",\n",
    "    \"icmodel_flux\",\"icmodel_flux_err\",\"icmodel_flux_flags\",\n",
    "    \"zcmodel_flux\",\"zcmodel_flux_err\",\"zcmodel_flux_flags\",\n",
    "    \"ycmodel_flux\",\"ycmodel_flux_err\",\"ycmodel_flux_flags\",\n",
    "]\n",
    "\n",
    "for col in photometry_cols:\n",
    "    combined[col] = HSC.df.loc[combined.catalog_2_ids][col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "low_z    = (combined.photo_z  < .15)\n",
    "low_mass = (combined.log_mass < 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Create classification labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Class A: matched **and** (low redshift + low mass)\n",
    "\n",
    "Class B: matched **but not** (low redshift + low mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class_a =  (low_z & low_mass)\n",
    "class_b = ~(low_z & low_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "combined[\"low_z_low_mass\"] = class_a\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Turn fluxes into rough colors\n",
    "Yes, I know these aren't exactly the right colors since I'm not including zero-points, but that shouldn't affect the results.\n",
    "\n",
    "(When I get a chance, I'll re-download the dataset so that it includes magnitudes not just fluxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "combined[\"g_minus_r\"] = -.4*np.log10(combined[\"gcmodel_flux\"] / combined[\"rcmodel_flux\"])\n",
    "combined[\"r_minus_i\"] = -.4*np.log10(combined[\"rcmodel_flux\"] / combined[\"icmodel_flux\"])\n",
    "combined[\"i_minus_z\"] = -.4*np.log10(combined[\"icmodel_flux\"] / combined[\"zcmodel_flux\"])\n",
    "combined[\"z_minus_y\"] = -.4*np.log10(combined[\"zcmodel_flux\"] / combined[\"ycmodel_flux\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For now, filter out bad photometry. Later I could consider passing this into the classifier, as an imputed/sentinel value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mask =    np.isfinite(combined[\"g_minus_r\"]) & np.isfinite(combined[\"r_minus_i\"]) \\\n",
    "        & np.isfinite(combined[\"i_minus_z\"]) & np.isfinite(combined[\"z_minus_y\"]) \\\n",
    "        & np.isfinite(combined[\"icmodel_flux\"]) \\\n",
    "        & (~combined.gcmodel_flux_flags) & (~combined.rcmodel_flux_flags) \\\n",
    "        & (~combined.icmodel_flux_flags) & (~combined.zcmodel_flux_flags) \\\n",
    "        & (~combined.ycmodel_flux_flags)\n",
    "\n",
    "combined = combined[mask]\n",
    "\n",
    "combined[\"log_icmodel_flux\"] = np.log10(combined[\"icmodel_flux\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = combined.loc[:,[\"g_minus_r\", \"r_minus_i\", \"i_minus_z\", \"z_minus_y\",\n",
    "                              \"log_icmodel_flux\"]]\n",
    "\n",
    "target = combined.loc[:,[\"low_z_low_mass\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "target.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Build Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Partition training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "testing_fraction = .1\n",
    "test_set_indices = np.random.choice(target.index.values, \n",
    "                                    replace=False,\n",
    "                                    size=int(testing_fraction*target.size)\n",
    "                                   )\n",
    "\n",
    "training_set_indices = np.array(list(set(target.index.values) - set(test_set_indices)))\n",
    "\n",
    "features_train = features.loc[training_set_indices]\n",
    "features_test  = features.loc[test_set_indices]\n",
    "\n",
    "target_train   = target.loc[training_set_indices]\n",
    "target_test    = target.loc[test_set_indices]\n",
    "\n",
    "true_a =  target_test.values.flatten()\n",
    "true_b = ~target_test.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_classification_characteristics(target_prob, threshold_prob, verbose=False):\n",
    "\n",
    "    target_prediction = (target_prob > threshold_prob)\n",
    "    \n",
    "    prediction_a =  target_prediction\n",
    "    prediction_b = ~target_prediction\n",
    "    \n",
    "    completeness = (true_a & prediction_a).sum() / (true_a).sum() \n",
    "    \n",
    "    purity = (true_a & prediction_a).sum() / (prediction_a).sum() \n",
    "    \n",
    "    sample_size_reduction = prediction_a.size / prediction_a.sum()\n",
    "    \n",
    "    true_positives  = np.sum(true_a & prediction_a)\n",
    "    false_positives = np.sum(true_b & prediction_a)\n",
    "    \n",
    "    true_negatives  = np.sum(true_b & prediction_b)\n",
    "    false_negatives = np.sum(true_a & prediction_b)\n",
    "    \n",
    "    true_positive_rate = true_positives / true_a.sum()\n",
    "    false_positive_rate = false_positives / true_b.sum()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"completeness:          \", completeness)\n",
    "        print(\"purity:                \", purity)\n",
    "        print(\"sample_size_reduction: \", sample_size_reduction)\n",
    "        print(\"true  positive rate:   \", true_positive_rate)\n",
    "        print(\"false positive rate:   \", false_positive_rate)\n",
    "        \n",
    "    return {\n",
    "        \"completeness\": completeness,\n",
    "        \"purity\": purity,\n",
    "        \"sample_size_reduction\": sample_size_reduction,\n",
    "        \"threshold_prob\": threshold_prob,\n",
    "        \"true_positive_rate\": true_positive_rate,\n",
    "        \"false_positive_rate\": false_positive_rate,\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier_RF = RandomForestClassifier()\n",
    "classifier_RF = classifier_RF.fit(features_train, target_train.values.flatten())\n",
    "\n",
    "target_prob_RF = classifier_RF.predict_proba(features_test)[:,1]\n",
    "print(\"min prob: \", target_prob_RF.min())\n",
    "print(\"max prob: \", target_prob_RF.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_classification_characteristics(target_prob_RF, .01, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "threshold_probs = np.linspace(0, 1, num=100)[1:-1]\n",
    "results_RF = [get_classification_characteristics(target_prob_RF, threshold_prob)\n",
    "              for threshold_prob in threshold_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "completenesses_RF         = [result[\"completeness\"] for result in results_RF]\n",
    "purities_RF               = [result[\"purity\"] for result in results_RF]\n",
    "sample_size_reductions_RF = [result[\"sample_size_reduction\"] for result in results_RF]\n",
    "true_positive_rates_RF    = [result[\"true_positive_rate\"] for result in results_RF]\n",
    "false_positive_rates_RF   = [result[\"false_positive_rate\"] for result in results_RF]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_LR = LogisticRegression(class_weight=\"balanced\")\n",
    "classifier_LR = classifier_LR.fit(features_train, np.array(target_train.values.flatten(), dtype=int))\n",
    "\n",
    "target_prob_LR = classifier_LR.predict_proba(features_test)[:,1]\n",
    "print(\"min prob: \", target_prob_LR.min())\n",
    "print(\"max prob: \", target_prob_LR.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_classification_characteristics(target_prob_LR, .01, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "threshold_probs = np.linspace(0, 1)[1:-1]\n",
    "results_LR = [get_classification_characteristics(target_prob_LR, threshold_prob)\n",
    "              for threshold_prob in threshold_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "completenesses_LR         = [result[\"completeness\"] for result in results_LR]\n",
    "purities_LR               = [result[\"purity\"] for result in results_LR]\n",
    "sample_size_reductions_LR = [result[\"sample_size_reduction\"] for result in results_LR]\n",
    "true_positive_rates_LR    = [result[\"true_positive_rate\"] for result in results_LR]\n",
    "false_positive_rates_LR   = [result[\"false_positive_rate\"] for result in results_LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "# Get specific galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "best_dwarfs_args  = np.argpartition(target_prob_RF, target_prob_RF.size-100)[-100:]\n",
    "worst_dwarfs_args = np.argpartition(target_prob_RF, 100)[:100]\n",
    "\n",
    "best_dwarfs_ids_cosmos  = target_test.iloc[best_dwarfs_args].index\n",
    "worst_dwarfs_ids_cosmos = target_test.iloc[worst_dwarfs_args].index\n",
    "\n",
    "best_dwarf_ids_hsc = combined.loc[best_dwarfs_ids_cosmos].catalog_2_ids\n",
    "worst_dwarf_ids_hsc = combined.loc[worst_dwarfs_ids_cosmos].catalog_2_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "random_ids_cosmos = np.random.choice(training_set_indices,\n",
    "                              replace=False,\n",
    "                              size=100,\n",
    "                             )\n",
    "\n",
    "random_ids_hsc = combined.loc[random_ids_cosmos].catalog_2_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Check: do any HSC ids overlap?\n",
    "By design the COSMOS ids shouldn't overlap, but the COSMOS id -> HSC id mapping isn't necessarily unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "set(best_dwarf_ids_hsc.values) & set(worst_dwarf_ids_hsc.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "set(best_dwarf_ids_hsc.values) & set(random_ids_hsc.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "set(worst_dwarf_ids_hsc.values) & set(random_ids_hsc.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Do they give reasonable dwarf fractions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "combined.loc[best_dwarfs_ids_cosmos].low_z_low_mass.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "combined.loc[worst_dwarfs_ids_cosmos].low_z_low_mass.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "combined.loc[random_ids_cosmos].low_z_low_mass.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Save the indices to disk\n",
    "(but only if they don't already exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_dir_quick_sample = os.path.join(dwarfz.data_dir_default, \"quick_sample\")\n",
    "if not os.path.exists(data_dir_quick_sample):\n",
    "    os.mkdir(data_dir_quick_sample)\n",
    "\n",
    "    np.savetxt(os.path.join(data_dir_quick_sample,\"ids_best.csv\"),   best_dwarf_ids_hsc.values,  fmt=\"%d\")\n",
    "    np.savetxt(os.path.join(data_dir_quick_sample,\"ids_worst.csv\"),  worst_dwarf_ids_hsc.values, fmt=\"%d\")\n",
    "    np.savetxt(os.path.join(data_dir_quick_sample,\"ids_random.csv\"), random_ids_hsc.values,      fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# What image size do I need?\n",
    "\n",
    "For this, you'll need to use `data.get_shapes.ipynb` to query + store the object shapes from the remote database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Build a list to send to Song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ids_best   = np.loadtxt(os.path.join(data_dir_quick_sample,\"ids_best.csv\"),\n",
    "                        dtype=int)\n",
    "ids_worst  = np.loadtxt(os.path.join(data_dir_quick_sample,\"ids_worst.csv\"),\n",
    "                        dtype=int)\n",
    "ids_random = np.loadtxt(os.path.join(data_dir_quick_sample,\"ids_random.csv\"),\n",
    "                        dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Check if any HSC ids are duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert( len(set(best_dwarf_ids_hsc)) == 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert( len(set(worst_dwarf_ids_hsc)) == 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "assert( len(set(random_ids_hsc)) == 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### But are multiple cosmos galaxies matched to any of those HSC id's?\n",
    "This will be a problem because there will be two masses / redshifts attached to a given HSC example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_best = combined[combined.catalog_2_ids.isin(best_dwarf_ids_hsc)]\n",
    "df_best.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_worst = combined[combined.catalog_2_ids.isin(worst_dwarf_ids_hsc)]\n",
    "df_worst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_random = combined[combined.catalog_2_ids.isin(random_ids_hsc)]\n",
    "df_random.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Disambiguate matches\n",
    "Simply select the closest of the COSMOS galaxies, and discard all others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def copy_and_filter_duplicates(df_old, verbose=False):\n",
    "    df = df_old.copy()\n",
    "    counts = Counter(df.catalog_2_ids)\n",
    "    \n",
    "    ambiguous_galaxy_hsc_ids = []\n",
    "    for hsc_id in counts:\n",
    "        if counts[hsc_id] > 1:\n",
    "            ambiguous_galaxy_hsc_ids.append(hsc_id)\n",
    "            if verbose:\n",
    "                print(hsc_id, counts[hsc_id])\n",
    "            \n",
    "            ambiguous_matches = df[df.catalog_2_ids == hsc_id]\n",
    "            better_match_cosmos_id = ambiguous_matches.sep.argmin()\n",
    "            worse_match_ids = set(ambiguous_matches.index) - set([better_match_cosmos_id])\n",
    "            \n",
    "            for worse_match_id in worse_match_ids:\n",
    "                df = df[df.index != worse_match_id ]\n",
    "        \n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_best = copy_and_filter_duplicates(df_best, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_worst = copy_and_filter_duplicates(df_worst, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_random = copy_and_filter_duplicates(df_random, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create a csv file for Song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir_quick_sample,\"galaxies_for_song.csv\"),\n",
    "          mode=\"w\") as f:\n",
    "    f.write(\"# object_id,ra,dec,z\\n\")\n",
    "    \n",
    "    kwargs = dict(\n",
    "        header=None,\n",
    "        index=False,\n",
    "        float_format=\"%lf\",\n",
    "    )\n",
    "    \n",
    "    columns = [\"catalog_2_ids\", \"ra\", \"dec\", \"photo_z\"]\n",
    "        \n",
    "    df_best[columns].to_csv(f, **kwargs)\n",
    "    \n",
    "    df_worst[columns].to_csv(f, **kwargs)\n",
    "    \n",
    "    df_worst[columns].to_csv(f, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!head {data_dir_quick_sample}/galaxies_for_song.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
