{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "The HSC data is too large to store as one sqlite database file using github.  So instead, it needs to be fetched by the user, separately from cloning the repository. This notebook is a work-in-progress to help automate that process, and make sure that the final schema is correct.\n",
    "\n",
    "\n",
    "## Sending the query\n",
    "The HSC data release site provides a [command line tool](https://hsc-gitlab.mtk.nao.ac.jp/snippets/17) for querying the database; I've adapted it to run programmatically from within a python session.  Check it out; it's the file `hsc_release_query.py`.  There's a working example of a simple query in `sql_tester.ipynb`. This notebook rolls everything together: querying the server, and combining the subsets into one table.\n",
    "\n",
    "## What gets saved?\n",
    "This comes in two parts:\n",
    "\n",
    " 1) Get the main HSC table (position, fluxes, flags for each object)\n",
    " \n",
    " 2) Get a list of matched spec-z's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Remember to set your credentials within `hsc_credentials.py` !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "# give access to importing dwarfz\n",
    "import os, sys\n",
    "dwarfz_package_dir = os.getcwd().split(\"dwarfz\")[0]\n",
    "if dwarfz_package_dir not in sys.path:\n",
    "    sys.path.insert(0, dwarfz_package_dir)\n",
    "\n",
    "import dwarfz\n",
    "from dwarfz.hsc_credentials import credential\n",
    "from dwarfz.hsc_release_query import query_wrapper\n",
    "    \n",
    "# back to regular import statements\n",
    "\n",
    "import os, sys\n",
    "import shutil\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get HSC Fluxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the query\n",
    "Right now it only gets the *fluxes*, not the magnitudes. So far, I haven't needed the zeropoint. But this is a good start place if you need to build a query that gets the magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_base = \"\"\"\n",
    "SELECT \n",
    "    object_id, \n",
    "    ra, dec, \n",
    "    detect_is_patch_inner, detect_is_tract_inner, detect_is_primary,\n",
    "    gcmodel_flux, gcmodel_flux_err, gcmodel_flux_flags,\n",
    "    rcmodel_flux, rcmodel_flux_err, rcmodel_flux_flags,\n",
    "    icmodel_flux, icmodel_flux_err, icmodel_flux_flags,\n",
    "    zcmodel_flux, zcmodel_flux_err, zcmodel_flux_flags,\n",
    "    ycmodel_flux, ycmodel_flux_err, ycmodel_flux_flags\n",
    "FROM \n",
    "    pdr1_cosmos_widedepth_median.forced\n",
    "LIMIT \n",
    "    {}\n",
    "OFFSET \n",
    "    {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the query\n",
    "**The total number of objects is currently hardcoded! Make sure this hasn't changed!**\n",
    "The cleaner way to do this would be to make a simple query to the database, then count the number of records. But for now, hardcoding it is simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_objects = 1263503\n",
    "\n",
    "block_size = 250000\n",
    "n_blocks = (n_objects // block_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_hsc_table_dir = \"partial_hsc_tables\"\n",
    "if not os.path.exists(temp_hsc_table_dir):\n",
    "    os.mkdir(temp_hsc_table_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "limit = block_size\n",
    "\n",
    "preview_results = False\n",
    "delete_job = True\n",
    "out_format = \"sqlite3\"\n",
    "\n",
    "for i in range(n_blocks):\n",
    "    offset = i*block_size\n",
    "    \n",
    "    sql = sql_base.format(limit, offset)\n",
    "    \n",
    "    output_filename = os.path.join(temp_hsc_table_dir, \"tmp_{}.sqlite3\".format(i))\n",
    "    \n",
    "    print(\" ---------------- QUERY {} -------------------- \".format(i+1))\n",
    "    print(sql)\n",
    "\n",
    "    with open(output_filename, mode=\"wb\") as output_file:\n",
    "        query_wrapper(credential, sql, preview_results, delete_job, \n",
    "                      out_format, output_file,\n",
    "                      nomail=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "database_filenames = sorted(glob.glob(os.path.join(temp_hsc_table_dir, \"tmp_*.sqlite3\")))\n",
    "database_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = [pd.read_sql_table(\"table_1\", \"sqlite:///{}\".format(database_filename),\n",
    "                         index_col=\"object_id\")\n",
    "       for database_filename in database_filenames]\n",
    "assert(sum(df.shape[0] for df in dfs) == n_objects)\n",
    "\n",
    "combined = pd.concat(dfs)\n",
    "assert(combined.shape[0] == n_objects)\n",
    "\n",
    "del dfs\n",
    "combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in database_filenames:\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hsc_database_filename = \"HSC_COSMOS_median_forced.sqlite3\"\n",
    "hsc_database_filename_old = hsc_database_filename + \".old\"\n",
    "\n",
    "if os.path.exists(hsc_database_filename):\n",
    "    try:\n",
    "        shutil.move(hsc_database_filename, hsc_database_filename_old)\n",
    "        combined.to_sql(\"hsc\", \"sqlite:///{}\".format(hsc_database_filename))\n",
    "    except:\n",
    "        # in case there's an error during writing, don't overwrite/delete the existing database\n",
    "        shutil.move(hsc_database_filename_old, hsc_database_filename)\n",
    "        raise\n",
    "    else:\n",
    "        # only delete if combining went successfully\n",
    "        os.remove(hsc_database_filename + \".old\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get spec-z's matched to HSC objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redshifts_sql = \"\"\"\n",
    "SELECT \n",
    "    object_id, specz_id,\n",
    "    d_pos, \n",
    "    specz_ra, specz_dec,\n",
    "    specz_redshift, specz_redshift_err, specz_flag_homogeneous\n",
    "FROM\n",
    "    pdr1_cosmos_widedepth_median.specz\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preview_results = False\n",
    "delete_job = True\n",
    "out_format = \"sqlite3\"\n",
    "\n",
    "output_filename = \"specz.{}\".format(out_format)\n",
    "print(output_filename)\n",
    "\n",
    "with open(output_filename, mode=\"wb\") as output_file:\n",
    "    query_wrapper(credential, redshifts_sql, preview_results, delete_job, \n",
    "                  out_format, output_file,\n",
    "                  nomail=True,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls -lh specz.sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_sql_table(\"table_1\", \n",
    "                            \"sqlite:///{}\".format(\"specz.sqlite3\"), \n",
    "                            index_col=\"object_id\")\n",
    "df = df[df.specz_flag_homogeneous]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
