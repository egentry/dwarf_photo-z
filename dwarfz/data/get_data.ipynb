{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "The HSC data is too large to store as one sqlite database file using github.  So instead, it needs to be fetched by the user, separately from cloning the repository. This notebook is a work-in-progress to help automate that process, and make sure that the final schema is correct.\n",
    "\n",
    "\n",
    "## Sending the query\n",
    "The HSC data release site provides a [command line tool](https://hsc-gitlab.mtk.nao.ac.jp/snippets/17) for querying the database; I've adapted it to run programmatically from within a python session.  Check it out; it's the file `hsc_release_query.py`.  There's a working example of a simple query in `sql_tester.ipynb`. This notebook rolls everything together: querying the server, and combining the subsets into one table.\n",
    "\n",
    "## What gets saved?\n",
    "This comes in two parts:\n",
    "\n",
    " 1) Get the main HSC table (position, fluxes, flags for each object)\n",
    " \n",
    " 2) Get a list of matched spec-z's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Remember to set your credentials within `hsc_credentials.py` !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "# give access to importing dwarfz\n",
    "import os, sys\n",
    "dwarfz_package_dir = os.getcwd().split(\"dwarfz\")[0]\n",
    "if dwarfz_package_dir not in sys.path:\n",
    "    sys.path.insert(0, dwarfz_package_dir)\n",
    "\n",
    "import dwarfz\n",
    "from dwarfz.hsc_credentials import credential\n",
    "from dwarfz.hsc_release_query import query_wrapper\n",
    "    \n",
    "# back to regular import statements\n",
    "\n",
    "import os, sys\n",
    "import shutil\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get HSC Fluxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the query\n",
    "Gets both the fluxes and the magnitudes. The difference shouldn't matter, but now you have both, depending on what's more convenient. In general, using the flux flags with the magnitude values is what I usually do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_base = \"\"\"\n",
    "SELECT \n",
    "    object_id, \n",
    "    ra, dec, \n",
    "    detect_is_patch_inner, detect_is_tract_inner, detect_is_primary,\n",
    "    gcmodel_flux, gcmodel_flux_err, gcmodel_flux_flags, gcmodel_mag,\n",
    "    rcmodel_flux, rcmodel_flux_err, rcmodel_flux_flags, rcmodel_mag,\n",
    "    icmodel_flux, icmodel_flux_err, icmodel_flux_flags, icmodel_mag,\n",
    "    zcmodel_flux, zcmodel_flux_err, zcmodel_flux_flags, zcmodel_mag,\n",
    "    ycmodel_flux, ycmodel_flux_err, ycmodel_flux_flags, ycmodel_mag\n",
    "FROM \n",
    "    pdr1_cosmos_widedepth_median.forced\n",
    "LIMIT \n",
    "    {}\n",
    "OFFSET \n",
    "    {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the query\n",
    "**The total number of objects is currently hardcoded! Make sure this hasn't changed!**\n",
    "The cleaner way to do this would be to make a simple query to the database, then count the number of records. But for now, hardcoding it is simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_objects = 1263503\n",
    "\n",
    "block_size = 250000\n",
    "n_blocks = (n_objects // block_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_hsc_table_dir = pathlib.Path(\"partial_hsc_tables\")\n",
    "if not temp_hsc_table_dir.is_dir():\n",
    "    temp_hsc_table_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "limit = block_size\n",
    "\n",
    "preview_results = False\n",
    "delete_job = True\n",
    "out_format = \"sqlite3\"\n",
    "\n",
    "for i in range(n_blocks):\n",
    "    offset = i*block_size\n",
    "    \n",
    "    sql = sql_base.format(limit, offset)\n",
    "    \n",
    "    output_filename = temp_hsc_table_dir /  \"tmp_{}.sqlite3\".format(i)\n",
    "    \n",
    "    print(\" ---------------- QUERY {} -------------------- \".format(i+1))\n",
    "    print(sql)\n",
    "\n",
    "    with open(output_filename, mode=\"wb\") as output_file:\n",
    "        query_wrapper(credential, sql, preview_results, delete_job, \n",
    "                      out_format, output_file,\n",
    "                      nomail=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_filenames = sorted(temp_hsc_table_dir.glob(\"tmp_*.sqlite3\"))\n",
    "database_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pd.read_sql_table(\"table_1\", \"sqlite:///{}\".format(database_filename),\n",
    "                         index_col=\"object_id\")\n",
    "       for database_filename in database_filenames]\n",
    "assert(sum(df.shape[0] for df in dfs) == n_objects)\n",
    "\n",
    "combined = pd.concat(dfs)\n",
    "assert(combined.shape[0] == n_objects)\n",
    "\n",
    "del dfs\n",
    "combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in database_filenames:\n",
    "    os.remove(filename)\n",
    "    \n",
    "if len(list(temp_hsc_table_dir.glob(\"*\")))==0:\n",
    "    temp_hsc_table_dir.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsc_database_filename = \"HSC_COSMOS_median_forced.sqlite3\"\n",
    "hsc_database_filename_old = hsc_database_filename + \".old\"\n",
    "\n",
    "if os.path.exists(hsc_database_filename):\n",
    "    try:\n",
    "        shutil.move(hsc_database_filename, hsc_database_filename_old)\n",
    "        combined.to_sql(\"hsc\", \"sqlite:///{}\".format(hsc_database_filename))\n",
    "    except:\n",
    "        # in case there's an error during writing, don't overwrite/delete the existing database\n",
    "        shutil.move(hsc_database_filename_old, hsc_database_filename)\n",
    "        raise\n",
    "    else:\n",
    "        # only delete if combining went successfully\n",
    "        os.remove(hsc_database_filename + \".old\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match HSC objects to COSMOS objects\n",
    "Every COSMOS galaxy will be in 1 pair. HSC galaxies can be in 0, 1 or more pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COSMOS_filename = pathlib.Path(dwarfz.data_dir_default) / \"COSMOS_reference.sqlite\"\n",
    "COSMOS = dwarfz.datasets.COSMOS(COSMOS_filename)\n",
    "COSMOS.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSC_filename = pathlib.Path(dwarfz.data_dir_default) / \"HSC_COSMOS_median_forced.sqlite3\"\n",
    "HSC = dwarfz.datasets.HSC(HSC_filename)\n",
    "HSC.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = dwarfz.matching.Matches(COSMOS.df, HSC.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_filename = pathlib.Path(dwarfz.data_dir_default) / \"matches.sqlite3\"\n",
    "if not matches_filename.exists():\n",
    "    matches.save_to_filename(matches_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"threshold (error) : {:>5.2f}\".format(matches.threshold_error))\n",
    "print(\"threshold (match) : {:>5.2f}\".format(matches.threshold_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"overall completeness : {:.2f} %\".format(100*np.mean(matches.df.match[~matches.df.error])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min separation: {:.4f} [arcsec]\".format(min(matches.df.sep)))\n",
    "print(\"max separation: {:.4f} [arcsec]\".format(max(matches.df.sep)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get spec-z's matched to HSC objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshifts_sql = \"\"\"\n",
    "SELECT \n",
    "    object_id, specz_id,\n",
    "    d_pos, \n",
    "    specz_ra, specz_dec,\n",
    "    specz_redshift, specz_redshift_err, specz_flag_homogeneous\n",
    "FROM\n",
    "    pdr1_cosmos_widedepth_median.specz\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_results = False\n",
    "delete_job = True\n",
    "out_format = \"sqlite3\"\n",
    "\n",
    "output_filename = \"specz.{}\".format(out_format)\n",
    "print(output_filename)\n",
    "\n",
    "with open(output_filename, mode=\"wb\") as output_file:\n",
    "    query_wrapper(credential, redshifts_sql, preview_results, delete_job, \n",
    "                  out_format, output_file,\n",
    "                  nomail=True,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh specz.sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_table(\"table_1\", \n",
    "                            \"sqlite:///{}\".format(\"specz.sqlite3\"), \n",
    "                            index_col=\"object_id\")\n",
    "df = df[df.specz_flag_homogeneous]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get FRANKEN-Z photo-z's, and then match to HSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the query\n",
    "There are no photo-z's with the \"fake\" COSMOS-field Wide images. That catalog was originally UltraDeep, degraded to being Wide-like.  To most-closely match the photo-z catalogs, I'd then want to look in the UltraDeep dataset; but to most-correctly prepare for running on the true-Wide data, I'll pull my photo-z's from the Deep later. (Note: no photo-z's have been publicly released for the Wide data within the COSMOS field, circa 8 June 2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photoz_sql = \"\"\"\n",
    "SELECT\n",
    "  pdr1_deep.forced.object_id,\n",
    "  pdr1_deep.forced.ra,\n",
    "  pdr1_deep.forced.dec,\n",
    "  pdr1_deep.photoz_frankenz.photoz_best,\n",
    "  pdr1_deep.photoz_frankenz.photoz_risk_best\n",
    "FROM\n",
    "  pdr1_deep.forced\n",
    "INNER JOIN pdr1_deep.photoz_frankenz \n",
    "    ON pdr1_deep.photoz_frankenz.object_id=pdr1_deep.forced.object_id\n",
    "WHERE (ra BETWEEN 149.25 AND 151.25) AND (dec BETWEEN 1.4 AND 3);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_results = False\n",
    "delete_job = True\n",
    "out_format = \"sqlite3\"\n",
    "\n",
    "output_filename = \"photoz_tmp.{}\".format(out_format)\n",
    "print(output_filename)\n",
    "\n",
    "with open(output_filename, mode=\"wb\") as output_file:\n",
    "    query_wrapper(credential, photoz_sql, preview_results, delete_job, \n",
    "                  out_format, output_file,\n",
    "                  nomail=True,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh photoz_tmp.sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_table(\"table_1\", \n",
    "                       \"sqlite:///{}\".format(\"photoz_tmp.sqlite3\"), \n",
    "                       index_col=\"object_id\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(\"FRANKENZ\", \"sqlite:///franken_z-DEEP-COSMOS.sqlite3\",\n",
    "          if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(\"photoz_tmp.sqlite3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross reference FRANKENZ ids to general HSC ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSC_filename = pathlib.Path(dwarfz.data_dir_default) /  \"HSC_COSMOS_median_forced.sqlite3\"\n",
    "HSC = dwarfz.datasets.HSC(HSC_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = dwarfz.matching.Matches(HSC.df, df )\n",
    "matches.df[\"HSC_ids\"] = matches.df.index\n",
    "matches.df[\"FRANKENZ_ids\"] = matches.df.catalog_2_ids\n",
    "\n",
    "matches.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSC.df.join(matches.df).join(df[[\"photoz_best\", \n",
    "                                 \"photoz_risk_best\"]], \n",
    "                             on=\"FRANKENZ_ids\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy index column to a new data frame, then only add desired columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSC_photo_zs = HSC.df.copy()[[]] # only copy index column\n",
    "HSC_photo_zs = HSC_photo_zs.join(matches.df[[\"FRANKENZ_ids\"]])\n",
    "HSC_photo_zs = HSC_photo_zs.join(df[[\"photoz_best\", \"photoz_risk_best\"]],\n",
    "                                 on=\"FRANKENZ_ids\")\n",
    "HSC_photo_zs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSC_photo_zs.to_sql(\"photo_z\", \n",
    "                    \"sqlite:///HSC_matched_to_FRANKENZ.sqlite\",\n",
    "                    if_exists=\"replace\",\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
