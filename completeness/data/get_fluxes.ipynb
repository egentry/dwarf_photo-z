{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "\n",
    "The HSC data is too large to store as one sqlite database file using github.  So instead, it needs to be fetched by the user, separately from cloning the repository. This notebook is a work-in-progress to help automate that process, and make sure that the final schema is correct.\n",
    "\n",
    "The one complication is that the database is also too large to fetch all-at-once, even if you just want ~10 columns rather than the full ~1000 columns. So you need to download it peicemeal, and then combine into a single database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    " **Remember to set your credentials within `hsc_credentials.py` !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from hsc_credentials import credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from hscReleaseQuery import query_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Build the query\n",
    "Right now it only gets the *fluxes*, not the magnitudes. So far, I haven't needed the zeropoint. But this is a good start place if you need to build a query that gets the magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sql_base = \"\"\"\n",
    "SELECT \n",
    "    object_id, \n",
    "    ra, dec, \n",
    "    detect_is_patch_inner, detect_is_tract_inner, detect_is_primary,\n",
    "    gcmodel_flux, gcmodel_flux_err, gcmodel_flux_flags,\n",
    "    rcmodel_flux, rcmodel_flux_err, rcmodel_flux_flags,\n",
    "    icmodel_flux, icmodel_flux_err, icmodel_flux_flags,\n",
    "    zcmodel_flux, zcmodel_flux_err, zcmodel_flux_flags,\n",
    "    ycmodel_flux, ycmodel_flux_err, ycmodel_flux_flags\n",
    "FROM \n",
    "    pdr1_cosmos_widedepth_median.forced\n",
    "LIMIT \n",
    "    {}\n",
    "OFFSET \n",
    "    {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Make the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The total number of objects is currently hardcoded! Make sure this hasn't changed!**\n",
    "The cleaner way to do this would be to make a simple query to the database, then count the number of records. But for now, hardcoding it is simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_objects = 1263503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "block_size = 250000\n",
    "n_blocks = (n_objects // block_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "limit = block_size\n",
    "\n",
    "preview_results = False\n",
    "delete_job = True\n",
    "out_format = \"sqlite3\"\n",
    "\n",
    "for i in range(n_blocks):\n",
    "    offset = i*block_size\n",
    "    \n",
    "    sql = sql_base.format(limit, offset)\n",
    "    \n",
    "    output_filename = \"tmp_{}.sqlite3\".format(i)\n",
    "    \n",
    "    print(\" ---------------- QUERY {} -------------------- \".format(i+1))\n",
    "    print(sql)\n",
    "\n",
    "    with open(output_filename, mode=\"wb\") as output_file:\n",
    "        query_wrapper(credential, sql, preview_results, delete_job, \n",
    "                      out_format, output_file,\n",
    "                      nomail=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "database_filenames = sorted(glob.glob(\"tmp_*.sqlite3\"))\n",
    "database_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfs = [pd.read_sql_table(\"table_1\", \"sqlite:///{}\".format(database_filename),\n",
    "                         index_col=\"object_id\")\n",
    "       for database_filename in database_filenames]\n",
    "assert(sum(df.shape[0] for df in dfs) == n_objects)\n",
    "\n",
    "combined = pd.concat(dfs)\n",
    "assert(combined.shape[0] == n_objects)\n",
    "\n",
    "del dfs\n",
    "combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for filename in database_filenames:\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hsc_database_filename = \"../HSC_COSMOS_median_forced.sqlite3\"\n",
    "hsc_database_filename_old = hsc_database_filename + \".old\"\n",
    "\n",
    "try:\n",
    "    shutil.move(hsc_database_filename, hsc_database_filename_old)\n",
    "    combined.to_sql(\"hsc\", \"sqlite:///{}\".format(hsc_database_filename))\n",
    "except:\n",
    "    shutil.move(hsc_database_filename_old, hsc_database_filename)\n",
    "    raise\n",
    "else:\n",
    "    os.remove(hsc_database_filename + \".old\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
